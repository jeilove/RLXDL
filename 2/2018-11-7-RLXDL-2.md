<img src="https://www.dropbox.com/s/maui7p7nm03tmu2/DLxRL-2imitation_ko.001.jpeg?raw=1">



안녕하세요. 강화학습을 공부하는 정원석이라고 합니다. 



오늘은 지난번 다루었던 Introduction 에 이어 

Slide :

<https://speakerdeck.com/wonseokjung/deep-reinforcement-learning-introduction>

Blog :

<https://github.com/wonseokjung/RLXDL/blob/master/1/2018-10-28-RLXDL-1.md>





지도 모방(Supervised Imitation)에 대하여 알아보도록 하겠습니다. 

모방학습(Imitation learning)이란 순차적인 의사결정(Sequential Decision Making)상황에서 지도학습 (Supervised learning) 방법을 사용하여 학습시키는 방법입니다. 



<img src="https://www.dropbox.com/s/f6avzmumy5dzsj0/DLxRL-2imitation_ko.003.jpeg?raw=1">

오늘 다룰 내용은 다음과 같습니다. 

1. 순차적인 결정문제 (Sequential decision problems)의 정의
2. 모방학습 (Imitation learning) : 의사결정을 내리기 위한 지도학습 
3.  딥모방학습의 최근 연구 사례들
4. 모방학습이 개선되어야 할 부분 







<img src="https://www.dropbox.com/s/tgm4lxj7cjf6sqd/DLxRL-2imitation_ko.004.jpeg?raw=1">

이번 챕터를 공부하면 기초적인 모방학습 알고리즘, 강점과 약점, 그리고 정의와 표기법에 대해서 배울 수 있습니다. 



<img src="https://www.dropbox.com/s/pba05rrs9pxi721/DLxRL-2imitation_ko.005.jpeg?raw=1">



먼저 용어와 표기법에 대해서 알아보도록 하겠습니다. 

이 그림과 같이 딥러닝 신경망을 통하여 어떠한 입력을 넣었을때 출력으로 레이블링된 오브젝트가 나옵니다. 

여기서 입력은 고양이 사진이며, 출력은 오브젝트의 레이블 입니다(고양이). 

예를 들어 호랑이 이미지를 인풋으로 넣게되면 기존에 학습된 모델에 의해 아웃풋으로 호랑이 레이블이 나올 것 입니다. 



<img src="https://www.dropbox.com/s/vms6do2anx3q7qb/DLxRL-2imitation_ko.006.jpeg?raw=1">

지도학습 (Supervised learning)에서는 출력이 레이블 이지만, 강화학습에서는 이 출력이 어떠한 행동 (action)이 될 수도 있습니다. 

호랑이가 입력으로 들어왔을때 잘 학습된 모델이라면 "도망간다."라는 행동을 할 확률이 가장 높게 나와야 할 것 입니다. 



<img src="https://www.dropbox.com/s/y526wqpm5t15q0g/DLxRL-2imitation_ko.008.jpeg?raw=1">

순차적인 의사(Sequential Decision)에서 행동을 선택해야 할때는, 이 입력값을 관찰 (Observation)이라고 합니다. 

인공신경망은 시간 $t$ 마다 관찰(observation) $o_t$를 입력으로 받은 뒤  파라미터 $\theta$ 를 통해 행동(action) $a_t$를  출력합니다. 

파라미터 $\theta$ 에의해 관찰 $o_t$ 일때 $a_t$ 를 선택하는 습성(behavior)을 정책(policy) , $\pi_\theta$ 라고 표기합니다. 

<img src="https://www.dropbox.com/s/y526wqpm5t15q0g/DLxRL-2imitation_ko.008.jpeg?raw=1">



입력이 무엇이냐에 따라 입력을  상태(state) $s_t$ 혹은 관찰(observation) $o_t$로 구분할 수 있습니다. 

상황에 따라서 옳은 행동을 하려면 그 상황을 알수 있는 정보가 많아야 할 텐데요. 

그 정도를 모두 알고 있다는 것을  상태(state) $s_t$ , 그렇지 않고 일부분 ( 예를들어 ''시각'') 관찰(observation) $o_t$ 라고 표기합니다. 

파라미터 $\theta$ 에의해 관찰 $o_t$ 일때 $a_t$ 를 선택하는 습성(behavior)을 정책(policy) , $\pi_\theta(a_t \mid o_t)$  으로 표기하며 

파라미터 $\theta$ 에의해 관찰 $s_t$ 일때 $a_t$ 를 선택하는 습성(behavior)을 정책(policy) , $\pi_\theta(a_t \mid s_t)$  으로 표기합니다. 



<img src="https://www.dropbox.com/s/434xqfmdou1e8na/DLxRL-2imitation_ko.009.jpeg?raw=1">

조금 더 상태와 관찰에 대해서 예를 통하여 자세히 알아보도록 하겠습니다. 

고양이가 쥐를 사냥하고 있는 상황을  상태(state) 와 관찰( observastion ) 으로 표현한다면, 



상태(state)는 세상을 근본으로 하여 얻을수 있는 정보 입니다. 

예를 들어 고양이와 쥐의 위치, 고양이의 속도, 쥐의 속도, 고양이의 종류, 털의 색, 쥐와 고양이의 크기 등 입니다. 



반면, 

관찰(observation)은 사진의 픽셀 입니다. 고양이의 털의 색, 크기 등 시각적으로 알수 있는 정보만을 가져올 수 있습니다. 



<img src="https://www.dropbox.com/s/ol2l3ojyrdg2lw3/DLxRL-2imitation_ko.010.jpeg?raw=1">



상태 정보는 ''세상을 요약'' 해놓은 정보와 같아, 이 정보를 사용하여 미래를 예측할 수도 있습니다. 

관찰은 상태를 기반으로 한 결과 및 정보이지만 손실 또한 있다고 가정합니다. 

집 안에 있는 고양이가 현재 세상의 정보라고 한다면, 

상태는  : 고양이의 위치, 털, 색, 집의 종류 크기 등 세상의 정보를 가지고 있어

고양이가 집에 들어가 있는 상황이라도 정보를 잃지 않는 반면,

관찰은 : 고양이의 일부분이 집에 가려 보이지 않아 정보를 그만큼 잃을 수 있습니다. 

<img src="https://www.dropbox.com/s/9dbh0h7m527qxp6/DLxRL-2imitation_ko.011.jpeg?raw=1">

상태, 관찰,행동의 관계를 모델로 그래프화(graphically model) 시킬수 있는데요. 

우리는 어떠한 행동을 할때 얻을수 있는 정보를 사용하여 행동을 선택합니다. 이때 얻을수 있는 정보는 손실된 정보 (관찰) 일 가능성이 많다고 가정합니다. 

그렇기 때문에  현재 시간 이전의 관찰을 사용하면. 현재의 관찰 정보만을 이용하는 것보다 더 많은 정보를 얻는다고 가정하여

현재 시간에서의 관찰값 뿐만 아닌 이전의 관찰값 또한 사용하여 행동을 합니다. 



<img src="https://www.dropbox.com/s/5oq9fb4mbbzfnxe/DLxRL-2imitation_ko.012.jpeg?raw=1">

인공신경망이 입력으로 이미지를 받고 행동을 출력하는 Imitation learning의 자율주행 자동차 예제를 하나 살펴보도록 하겠습니다. 

사람이 운전을 하며  대시보드에 있는 카메라를 통해 사진정보와 핸들의 움직임 데이터를 모읍니다. 

여기서의 관찰은 카메라에서 얻을 수 있는 이미지며, 출력은 핸들의 움직임 입니다. 



<img src="https://www.dropbox.com/s/7pu72n50zu9nb19/DLxRL-2imitation_ko.013.jpeg?raw=1">



1. 이렇게 사람의 행동을 따라하는 것을 습성 복제 ( behavior cloning )라고 하는데, 

2. 먼저 사람이 자동차를 운전하고, 대쉬보드에 있는 카메라를 통해 화면을 녹화합니다. 

3. 자동차 핸들의 움직임과 핸들 화면을 녹화한것을 인코딩합니다. 
4. 앞 배경 화면을 녹화한것과 운전대의 움직임을 녹화한것의 데이터를 모으고, 
5. 그것을 사용하여 지도학습을 합니다. 

<img src="https://www.dropbox.com/s/h6o90ztndavelue/DLxRL-2imitation_ko.014.jpeg?raw=1">



하지만 이 방법이 항상 잘 되지는 않는데요. 

이유는, 트레이닝 알고리즘과 데이터가 완벽할수 없고, 학습 과정중 아주 약간의 실수가 있다고 발생하면 

그 실수로  인해 전혀 다른 정책으로 학습이 되어버리기 때문입니다. 















<img src="https://www.dropbox.com/s/21cc2qt3kqt3yrv/DLxRL-2imitation_ko.016.jpeg?raw=1">

그래서 그 문제를 해결하기 위한 하나의 방법으로, 



여러 번의 궤도 (trajectory)를 통해 분포(distribution)을 만들어주어서 그 분포 안에서 행동을 할 수 있도록 만들어줍니다. 

<img src="https://www.dropbox.com/s/11x3y8nou4pizt0/DLxRL-2imitation_ko.017.jpeg?raw=1">

또 학습을 잘하게 하는 다른 방법으로, 

정책을 학습할때는  관찰의 분포( pdata) 에서 학습을 합니다. 

<img src="https://www.dropbox.com/s/12gzjajxw6yug1c/DLxRL-2imitation_ko.018.jpeg?raw=1">

여기서 가정하는 것은, 

1. 학습된 정책을 통하여 얻은 관찰의 분포는 사람이 운전하여 얻은 pdata 와는 다르다. 

2. 모든 문제점은 pdata와 정책을 통해 얻은 데이터가 다르기 때문에 발생할 것이다. 

3. 그렇다면 pdata와 학습된 정책을 통해 만들어진  data를 같게 해보자 ! 





<img src="https://www.dropbox.com/s/h9bqb8vr9rfg4h1/DLxRL-2imitation_ko.019.jpeg?raw=1">

pdata와 학습된 정책을 통해 만들어진  data를 같게 하기 위해 pdata를 바꿔보려고 시도를 하는데요. 



<img src="https://www.dropbox.com/s/rgx2ryujcagtcwg/DLxRL-2imitation_ko.020.jpeg?raw=1">

그에 관련된 연구로 DAgger: Dataset Aggregation이라는 연구에서는, 

정책으로부터 얻은 데이터를 사람이 다시 행동을 수정해줍니다.



<img src="https://www.dropbox.com/s/asmjcju1g27ynkc/DLxRL-2imitation_ko.021.jpeg?raw=1">

좀 더 자세히 살펴보면,

1. 사람이 운전하며 모은 데이터를 통하여 정책을 학습하고,
2. 그 정책을 사용하여 데이터를 모읍니다. 
3. 그렇게 모여진 데이터를 사람이 수정을 하고
4. 그것을 기존에 사람이 만든 데이터와 합칩니다.
5. 이 과정을 계속 반복하며 좀 더 정교하게 데이터와 정책을 만듭니다. 

<img src="https://www.dropbox.com/s/0c6whp85lzfay4w/DLxRL-2imitation_ko.022.jpeg?raw=1">



이러한 이미테이션러닝에 문제점은 무엇이 있을까요? 

첫번째로 사람이 데이터를 모아야하기 때문에 데이터가 한정적입니다. 

두번째는 사람이 수정할수 있는 행동은 한계가 있습니다. 예를 들어 사람이 아닌 드론의 행동을 수정할때는 

비교적 부정확할 것입니다. 

세번째 의문점은, 사람이 학습을 할때는 자주적으로 학습하는데 기계는 그렇게 하지 못한다는 것 입니다. 

그래서 사람이 항상 개입해줘야한다는 불편한점이 생깁니다. 





<img src="https://www.dropbox.com/s/ggvbwfm5ufac774/DLxRL-2imitation_ko.023.jpeg?raw=1">

다음 챕터에서는 이러한 이슈들을 보안할수 있는 방법으로 강화학습을 소개합니다. 
( 하지만 사실 이미테이션러닝은 강화학습에서 발생하는 학습속도, 자연스러운 움직임을 해결하기 위해 함게 사용되고 있습니다. )



그럼 다음주에 다시 뵙겠습니다. 헥헥 
